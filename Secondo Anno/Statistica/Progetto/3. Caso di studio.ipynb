{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <img src=\"https://digip.unibg.it/sites/dip3/files/logo-dip-it.svg\">\n",
    "  <h1><b>Crash course in Python </b> - AA 2024/2025</h1>\n",
    "  <h2>Lezione 3 - Caso di studio ‚úèÔ∏è</h2>\n",
    "  <h3>A cura di <a href=\"https://github.com/lamferzon?tab=repositories\">Lorenzo Leoni</a></h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione dei pacchetti üìÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione dei dati üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"phone_data.csv\")\n",
    "\n",
    "# rinominazione delle colonne\n",
    "data = data.rename(\n",
    "    columns={col_i: col_i.lower().replace(\" \", \"_\") for col_i in data.columns}\n",
    ")\n",
    "\n",
    "# alcune informazioni riguardanti il dataset\n",
    "nrows, ncols = data.shape\n",
    "print(f\"Dimensioni del dataset \\n- Numero di righe: {nrows}.\\n- Numero di colonne: {ncols}.\")\n",
    "print(\"\\nColonne del dataset e rispettivi tipi\")\n",
    "for col_i in data.columns:\n",
    "    print(f\"- {col_i.capitalize()} ({data[col_i].dtype}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborazione preliminare üîß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rimozione delle colonne superflue o ad alta cardinalit√†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cardinalit√† delle colonne categoriche\")\n",
    "for col_i in data.columns:\n",
    "    if data[col_i].dtype == object:\n",
    "        print(f\"- {col_i.capitalize()}: {len(data[col_i].unique())}.\")\n",
    "\n",
    "data = data.drop(\n",
    "    columns=[\n",
    "        \"phone_model\",\n",
    "        \"price\",\n",
    "        \"currency\",\n",
    "        \"launch\",\n",
    "        \"display_type\",\n",
    "        \"display_resolution\",\n",
    "        \"os\",\n",
    "        \"usb\",\n",
    "        \"features_sensors\",\n",
    "        \"colors\",\n",
    "        \"video\",\n",
    "        \"gpu\",\n",
    "        \"quantile_10\",\n",
    "        \"quantile_50\",\n",
    "        \"quantile_90\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione della colonna `volume`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volume(row):\n",
    "    match = re.findall(r\"([\\d.]+) x ([\\d.]+) x ([\\d.]+) mm\", row.dimensions)\n",
    "    if match:\n",
    "        height, width, thickness = match[0]\n",
    "        return round(float(height)*float(width)*float(thickness), 2)\n",
    "    else:\n",
    "        None\n",
    "data[\"volume\"] = data.apply(get_volume, axis=1)\n",
    "\n",
    "# rimozione della colonna \"dimensions\" e dei dati mancanti\n",
    "data = data.drop(\n",
    "    columns=[\n",
    "        \"dimensions\"\n",
    "    ]\n",
    ").dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione della colonna `chipset_brand`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chipset_brand(row):\n",
    "    return row.chipset.split()[0]\n",
    "data[\"chipset_brand\"] = data.apply(get_chipset_brand, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione della colonna `chipset_production_process`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chipset_production_process(row):\n",
    "    match = re.search(r\"\\((\\d+)\\s*nm\\)\", row.chipset)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        None\n",
    "data[\"chipset_production_process\"] = data.apply(get_chipset_production_process, axis=1)\n",
    "\n",
    "# rimozione della colonna \"chipset\" e dei dati mancanti\n",
    "data = data.drop(\n",
    "    columns=[\n",
    "        \"chipset\"\n",
    "    ]\n",
    ").dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione della colonna `cpu_cores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_map = {\n",
    "    'Hexa': 6,\n",
    "    'Octa': 8,\n",
    "    'Quad': 4,\n",
    "    'Nona': 9,\n",
    "    'Deca': 10,\n",
    "    'Dual': 2\n",
    "}\n",
    "\n",
    "def get_cpu_cores(row):\n",
    "    \n",
    "    # primo passa: ricerca dei prefissi come \"Hexa\", \"Octa\", \"Quad\", etc.\n",
    "    for key, value in core_map.items():\n",
    "        if key.lower() in row.cpu.lower():\n",
    "            return int(value)\n",
    "    \n",
    "    # secondo passo: utilizzo di un'espressione regolare (regex) per cercare numeri seguiti da \"-core\"\n",
    "    match = re.search(r'(\\d+)-core', row.cpu, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "data[\"cpu_cores\"] = data.apply(get_cpu_cores, axis=1)\n",
    "\n",
    "# rimozione della colonna \"cpu\" e dei dati mancanti\n",
    "data = data.drop(\n",
    "    columns=[\n",
    "        \"cpu\"\n",
    "    ]\n",
    ").dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversione di tipo delle colonne `nfc`, `foldable` e `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"phone_brand\"] = data.phone_brand.str.capitalize()\n",
    "data[\"nfc\"] = data.nfc.replace(1, \"Yes\").replace(0, \"Not\")\n",
    "data[\"foldable\"] = data.foldable.replace(1, \"Yes\").replace(0, \"Not\")\n",
    "data[\"year\"] = data.year.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificazione delle variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = \"price_usd\"\n",
    "num_vars = [var_i for var_i in data.columns if ((data[var_i].dtype == float) | (data[var_i].dtype == int)) & (var_i != target_var)]\n",
    "cat_vars = [var_i for var_i in data.columns if data[var_i].dtype == object]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame dopo il processamento iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_index(axis=1).sort_values(by=\"price_usd\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi preliminare üî¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ncols=2,\n",
    "    nrows=3,\n",
    "    figsize=(15, 18)\n",
    ")\n",
    "fig.subplots_adjust(\n",
    "    wspace=.2,\n",
    "    hspace=.3\n",
    ")\n",
    "\n",
    "# andamento temporale del prezzo medio, per segmento di mercato\n",
    "sns.lineplot(\n",
    "    data=data,\n",
    "    x=\"year\",\n",
    "    y=\"price_usd\",\n",
    "    ls=\"-.\",\n",
    "    hue=\"price_range\",\n",
    "    palette=\"bright\",\n",
    "    ax=axes[0, 0]\n",
    ")\n",
    "axes[0, 0].axvline(\n",
    "    \"2020\",\n",
    "    color=\"red\",\n",
    "    ls=\"--\",\n",
    "    alpha=.75,\n",
    "    label=\"Pandemia\"\n",
    ")\n",
    "axes[0, 0].set_title(\n",
    "    \"$\\\\mathbf{Andamento \\\\ temporale \\\\ del \\\\ prezzo \\\\ medio \\\\ di \\\\ vendita}$\\n per segmento di mercato\",\n",
    "    size=11\n",
    ")\n",
    "axes[0, 0].set(\n",
    "    xlabel=\"Anno\",\n",
    "    ylabel=\"Prezzo di vendita[$]\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[0, 0].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "axes[0, 0].legend(\n",
    "    title=\"$\\\\mathbf{Segmento}$\"\n",
    ")\n",
    "\n",
    "# andamento temporale del prezzo medio, per negozio online\n",
    "sns.lineplot(\n",
    "    data=data,\n",
    "    x=\"year\",\n",
    "    y=\"price_usd\",\n",
    "    hue=\"store\",\n",
    "    style=\"store\",\n",
    "    palette=\"bright\",\n",
    "    err_style=None,\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    ax=axes[0, 1]\n",
    ")\n",
    "axes[0, 1].set_title(\n",
    "    \"$\\\\mathbf{Andamento \\\\ temporale \\\\ del \\\\ prezzo \\\\ medio \\\\ di \\\\ vendita}$\\nper negozio online\",\n",
    "    size=11\n",
    ")\n",
    "axes[0, 1].set(\n",
    "    xlabel=\"Anno\",\n",
    "    ylabel=\"Prezzo di vendita [$]\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[0, 1].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "axes[0, 1].legend(\n",
    "    title=\"$\\\\mathbf{Negozio \\\\ online}$\"\n",
    ")\n",
    "\n",
    "# andamento temporale del prezzo medio, per produttore\n",
    "sns.lineplot(\n",
    "    data=data[\n",
    "        (data.phone_brand == \"Samsung\") | \n",
    "        (data.phone_brand == \"Google\") |\n",
    "        (data.phone_brand == \"Apple\") |\n",
    "        (data.phone_brand == \"Xiaomi\")\n",
    "    ],\n",
    "    x=\"year\",\n",
    "    y=\"price_usd\",\n",
    "    hue=\"phone_brand\",\n",
    "    palette=\"bright\",\n",
    "    err_style=\"bars\",\n",
    "    ax=axes[1, 0]\n",
    ")\n",
    "axes[1, 0].set_title(\n",
    "    \"$\\\\mathbf{Andamento \\\\ temporale \\\\ del \\\\ prezzo \\\\ medio \\\\ di \\\\ vendita}$\\nper produttore\",\n",
    "    size=11\n",
    ")\n",
    "axes[1, 0].set(\n",
    "    xlabel=\"Anno\",\n",
    "    ylabel=\"Prezzo di vendita [$]\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[1, 0].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "axes[1, 0].legend(\n",
    "    title=\"$\\\\mathbf{Produttore}$\"\n",
    ")\n",
    "\n",
    "# heatmap del prezzo medio di vendita, per produttore\n",
    "sns.heatmap(\n",
    "    data.pivot_table(\n",
    "        index=\"phone_brand\",\n",
    "        columns=\"year\",\n",
    "        values=\"price_usd\",\n",
    "        aggfunc=\"mean\"\n",
    "    ),\n",
    "    annot=True,\n",
    "    fmt=f\".0f\",\n",
    "    cbar=False,\n",
    "    ax=axes[1, 1]\n",
    ")\n",
    "axes[1, 1].set_title(\n",
    "    \"$\\\\mathbf{Heatmap\\\\ del \\\\ prezzo \\\\ medio \\\\ di \\\\ vendita}$\\nper produttore\",\n",
    "    size=11\n",
    ")\n",
    "axes[1, 1].set(\n",
    "    xlabel=\"Anno\",\n",
    "    ylabel=None\n",
    ")\n",
    "\n",
    "# distribuzione del prezzo medio, per segmento di mercato\n",
    "sns.boxenplot(\n",
    "    data=data,\n",
    "    x=\"price_usd\",\n",
    "    hue=\"price_range\",\n",
    "    palette=\"bright\",\n",
    "    ax=axes[2, 0]\n",
    ")\n",
    "axes[2, 0].set_title(\n",
    "    \"$\\\\mathbf{Distribuzione \\\\ del \\\\ prezzo \\\\ di \\\\ vendita}$\\nper segmento di mercato\",\n",
    "    size=11\n",
    ")\n",
    "axes[2, 0].set(\n",
    "    xlabel=\"Prezzo di vendita [$]\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[2, 0].grid(\n",
    "    axis=\"x\",\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "axes[2, 0].legend(\n",
    "    title=\"$\\\\mathbf{Segmento}$\"\n",
    ")\n",
    "\n",
    "# andamento temporale del numero di modelli rilasciati, per segmento di mercato (solo Amazon UK)\n",
    "sns.lineplot(\n",
    "    data=data[data[\"store\"] == \"Amazon UK\"].groupby([\"year\", \"price_range\"]).agg(count=(\"price_range\", \"count\")).reset_index(),\n",
    "    x=\"year\",\n",
    "    y=\"count\",\n",
    "    hue=\"price_range\",\n",
    "    palette=\"bright\",\n",
    "    alpha=.5,\n",
    "    legend=False,\n",
    "    ax=axes[2, 1]\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=data[data[\"store\"] == \"Amazon UK\"].groupby([\"year\", \"price_range\"]).agg(count=(\"price_range\", \"count\")).reset_index(),\n",
    "    x=\"year\",\n",
    "    y=\"count\",\n",
    "    hue=\"price_range\",\n",
    "    palette=\"bright\",\n",
    "    ax=axes[2, 1]\n",
    ")\n",
    "axes[2, 1].set_title(\n",
    "    \"$\\\\mathbf{Andamento \\\\ temporale \\\\ del \\\\ numero \\\\ di \\\\ modelli \\\\ rilasciati}$\\nper segmento di mercato (solo Amazon UK)\",\n",
    "    size=11\n",
    ")\n",
    "axes[2, 1].set(\n",
    "    xlabel=\"Anno\",\n",
    "    ylabel=\"Numero di modelli rilasciati\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[2, 1].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "axes[2, 1].legend(\n",
    "    title=\"$\\\\mathbf{Segmento}$\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trasformazioni e rimozione degli outlier ‚õî"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applicazione del logaritmo naturale\n",
    "\n",
    "### **Obiettivo**\n",
    "Convertire le variabili numeriche con distribuzioni asimmetriche in distribuzioni pi√π simmetriche, rendendo le relazioni tra variabili pi√π lineari.\n",
    "\n",
    "### **Formula**\n",
    "$x_{\\text{trasformato}} = \\log(x + c)$\n",
    "- $x$ √® il valore originale.\n",
    "- $c$ √® una costante per evitare $\\log(0)$ (solitamente $c=[0, 1]$).\n",
    "\n",
    "### **Note** \n",
    "- Riduce la varianza.\n",
    "- Evidenzia le relazioni lineari nascoste.\n",
    "- Applicabile solo a valori positivi. Per gestire valori nulli o negativi, √® necessario aggiungere una costante $c > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obiettivo**\n",
    "Rendere le variabili numeriche comparabili, eliminando gli effetti delle diverse scale.\n",
    "\n",
    "### **Formula:**\n",
    "$x_{\\text{standardizzato}} = \\frac{x - \\mu}{\\sigma}$\n",
    "- $x$ √® il valore originale.\n",
    "- $\\mu$ √® la media della variabile.\n",
    "- $\\sigma$ √® la deviazione standard della variabile.\n",
    "\n",
    "### **Note** \n",
    "  - Centra le variabili attorno a zero.\n",
    "  - Rende la varianza unitaria.\n",
    "  - Evita che variabili con scale maggiori dominino il modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = data.copy()\n",
    "scaler = {}\n",
    "\n",
    "for var_i in num_vars + [target_var]:\n",
    "    \n",
    "    # applicazione del logaritmo\n",
    "    x = np.log(np.array(tr_data[var_i]).reshape(-1, 1))\n",
    "\n",
    "    # standardizzazione\n",
    "    scaler[var_i] = StandardScaler()\n",
    "    tr_data[var_i] = scaler[var_i].fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rimozione degli outlier tramite IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obiettivo**\n",
    "Eliminare valori estremi che potrebbero distorcere i risultati della regressione.\n",
    "\n",
    "### **Procedimento**\n",
    "  - $\\text{IQR} = Q3 - Q1$.\n",
    "  - $\\text{lower-bound} = Q1 - 1.5 \\times \\text{IQR}$.\n",
    "  - $\\text{upper-bound} = Q3 + 1.5 \\times \\text{IQR}$.\n",
    "  - Rimuovere i valori $x$ che soddisfano $x < Q1 - (1.5 \\cdot \\text{IQR}) \\ \\text{oppure} \\ x > Q3 + (1.5 \\cdot \\text{IQR})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_i in num_vars + [target_var]:\n",
    "    \n",
    "    if var_i != \"cpu_cores\":\n",
    "        \n",
    "        # calcolo dei quantili e dell'IQR\n",
    "        Q1 = tr_data[var_i].quantile(0.25)\n",
    "        Q3 = tr_data[var_i].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # filtraggio\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        tr_data = tr_data[(tr_data[var_i] >= lower_bound) & (tr_data[var_i] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confronto tra le distribuzioni dei dati originali e trasformati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = {\n",
    "    \"weight\": \"Peso [g]\",\n",
    "    \"display_size\": \"Diagonale del display [inch]\",\n",
    "    \"volume\": \"Volume [mm$^3$]\",\n",
    "    \"chipset_production_process\": \"Processo produttivo [nm]\",\n",
    "    \"cpu_cores\": \"Numero di core della CPU\",\n",
    "    \"price_usd\": \"Prezzo di vendita [$]\"\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=4,\n",
    "    nrows=len(num_vars + [target_var]),\n",
    "    figsize=(18, 4.5*len(num_vars + [target_var]))\n",
    ")\n",
    "fig.subplots_adjust(\n",
    "    wspace=.3,\n",
    "    hspace=.45\n",
    ")\n",
    "\n",
    "for i, var_i in enumerate(num_vars + [target_var]):\n",
    "    \n",
    "    # istogramma dei dati originali\n",
    "    sns.histplot(\n",
    "        data=data,\n",
    "        x=var_i,\n",
    "        color=\"dodgerblue\",\n",
    "        stat=\"percent\",\n",
    "        bins=20,\n",
    "        ax=axes[i, 0]\n",
    "    )\n",
    "    axes[i, 0].set_title(\n",
    "        \"$\\\\mathbf{Istogramma \\\\ dei \\\\ dati \\\\ originali}$\\n\" + map[var_i],\n",
    "        size=11\n",
    "    )\n",
    "    axes[i, 0].set(\n",
    "        xlabel=map[var_i],\n",
    "        ylabel=\"Percentuale [%]\",\n",
    "        facecolor=\"whitesmoke\"\n",
    "    )\n",
    "    axes[i, 0].grid(\n",
    "        ls=\"--\",\n",
    "        alpha=.5\n",
    "    )\n",
    "    \n",
    "    # distribuzione dei dati originali\n",
    "    src_mu = round(data[var_i].mean(), 2)\n",
    "    src_std = round(data[var_i].std(), 2)\n",
    "    src_median = round(data[var_i].median(), 2)\n",
    "    sns.boxenplot(\n",
    "        data=data,\n",
    "        x=var_i,\n",
    "        color=\"dodgerblue\",\n",
    "        ax=axes[i, 1]\n",
    "    )\n",
    "    axes[i, 1].axvline(\n",
    "        src_median,\n",
    "        color=\"limegreen\",\n",
    "        ls=\"--\",\n",
    "        label=\"Mediana: \" + str(src_median)\n",
    "    )\n",
    "    axes[i, 1].set_title(\n",
    "        \"$\\\\mathbf{Distribuzione \\\\ dei \\\\ dati \\\\ originali}$\\n$\\\\mu=\" + str(src_mu) + \"$, $\\\\sigma=\" + str(src_std) + \"$\",\n",
    "        size=11\n",
    "    )\n",
    "    axes[i, 1].set(\n",
    "        xlabel=map[var_i],\n",
    "        facecolor=\"whitesmoke\"\n",
    "    )\n",
    "    axes[i, 1].grid(\n",
    "        axis=\"x\",\n",
    "        ls=\"--\",\n",
    "        alpha=.5\n",
    "    )\n",
    "    axes[i, 1].legend(\n",
    "        loc=\"upper right\"\n",
    "    )\n",
    "    \n",
    "    # istogramma dei dati trasformati\n",
    "    sns.histplot(\n",
    "        data=tr_data,\n",
    "        x=var_i,\n",
    "        color=\"orangered\",\n",
    "        stat=\"percent\",\n",
    "        bins=20,\n",
    "        kde=True,\n",
    "        ax=axes[i, 2]\n",
    "    )\n",
    "    axes[i, 2].set_title(\n",
    "        \"$\\\\mathbf{Istogramma \\\\ dei \\\\ dati \\\\ trasformati}$\\nOutlier filtrati\",\n",
    "        size=11\n",
    "    )\n",
    "    axes[i, 2].set(\n",
    "        xlabel=map[var_i],\n",
    "        ylabel=\"Percentuale [%]\",\n",
    "        facecolor=\"whitesmoke\"\n",
    "    )\n",
    "    axes[i, 2].grid(\n",
    "        ls=\"--\",\n",
    "        alpha=.5\n",
    "    )\n",
    "    \n",
    "    # distribuzione dei dati trasformati\n",
    "    tr_mu = round(tr_data[var_i].mean(), 2)\n",
    "    tr_std = round(tr_data[var_i].std(), 2)\n",
    "    tr_median = round(tr_data[var_i].median(), 2)\n",
    "    sns.boxenplot(\n",
    "        data=tr_data,\n",
    "        x=var_i,\n",
    "        color=\"orangered\",\n",
    "        ax=axes[i, 3]\n",
    "    )\n",
    "    axes[i, 3].axvline(\n",
    "        tr_median,\n",
    "        color=\"limegreen\",\n",
    "        ls=\"--\",\n",
    "        label=\"Mediana: \" + str(tr_median)\n",
    "    )\n",
    "    axes[i, 3].set_title(\n",
    "        \"$\\\\mathbf{Distribuzione \\\\ dei \\\\ dati \\\\ trasformati}$\\n$\\\\mu=\" + str(tr_mu) + \"$, $\\\\sigma=\" + str(tr_std) + \"$\",\n",
    "        size=11\n",
    "    )\n",
    "    axes[i, 3].set(\n",
    "        xlabel=map[var_i],\n",
    "        facecolor=\"whitesmoke\"\n",
    "    )\n",
    "    axes[i, 3].grid(\n",
    "        axis=\"x\",\n",
    "        ls=\"--\",\n",
    "        alpha=.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlazioni prima e dopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ncols=2,\n",
    "    nrows=1,\n",
    "    figsize=(15, 6)\n",
    ")\n",
    "fig.subplots_adjust(\n",
    "    wspace=.5\n",
    ")\n",
    "\n",
    "# correlazioni tra le variabili originali\n",
    "sns.heatmap(\n",
    "    data[num_vars + [target_var]].corr(),\n",
    "    cbar=False,\n",
    "    annot=True,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\n",
    "    \"$\\\\mathbf{Correlazioni \\\\ tra \\\\ le \\\\ variabili \\\\ originali}$\",\n",
    "    size=11,\n",
    "    y=1.025\n",
    ")\n",
    "axes[0].set(\n",
    "    yticklabels=map.values(),\n",
    "    xticklabels=[]\n",
    ")\n",
    "\n",
    "# correlazioni tra le variabili trasformate\n",
    "sns.heatmap(\n",
    "    tr_data[num_vars + [target_var]].corr(),\n",
    "    cbar=False,\n",
    "    annot=True,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"$\\\\mathbf{Correlazioni \\\\ tra \\\\ le \\\\ variabili \\\\ trasformate}$\",\n",
    "    size=11,\n",
    "    y=1.025\n",
    ")\n",
    "axes[1].set(\n",
    "    yticklabels=map.values(),\n",
    "    xticklabels=[]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scelta dei regressori üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione per costruire la formula in stile R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_formula(t_var, n_vars, c_vars):\n",
    "    \n",
    "    num_part = \" + \".join(n_vars)\n",
    "    cat_part = \" + \".join([f\"C({var_i})\" for var_i in c_vars])\n",
    "    formula_parts = [num_part, cat_part]\n",
    "    formula = f\"{t_var} ~ \" + \" + \".join([part_i for part_i in formula_parts if part_i])\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suddivisione del dataset in train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_train_data, tr_test_data = train_test_split(tr_data, train_size=.8, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise backward\n",
    "L'approccio **stepwise backward** √® una tecnica di selezione delle variabili utilizzata per costruire modelli statistici, come la regressione lineare, in modo da migliorare la parsimonia e ridurre l'overfitting. Questo metodo rimuove iterativamente le variabili meno significative (quelle che peggiorano maggiormente la qualit√† del modello) finch√© non si raggiunge una configurazione ottimale.\n",
    "\n",
    "### **Procedimento**\n",
    "1. **Si inizia con il modello completo**: si parte con un modello che include tutte le variabili (sia numeriche che categoriali) disponibili.\n",
    "2. **Calcolo della metrica di selezione**: per ogni iterazione, viene calcolata una metrica che misura la bont√† del modello, come l'`AIC (Akaike Information Criterion)` o il `BIC (Bayesian Information Criterion)`. Questi criteri bilanciano la qualit√† dell'adattamento e la complessit√† del modello.\n",
    "3. **Rimozione della variabile meno significativa**: si esamina ogni variabile nel modello e si calcola l'impatto della sua rimozione. Si rimuove la variabile che comporta il miglior miglioramento del criterio selezionato (ad esempio, il valore AIC pi√π basso).\n",
    "4. **Iterazione**: il processo viene ripetuto fino a quando non √® possibile rimuovere altre variabili senza peggiorare significativamente la qualit√† del modello. Il processo si ferma quando l'eliminazione di ulteriori variabili non porta a un miglioramento del modello.\n",
    "5. **Modello finale**: il risultato finale √® un modello con un numero ridotto di variabili che sono considerate le pi√π rilevanti.\n",
    "\n",
    "### **Note**\n",
    "- Riduce il rischio di overfitting.\n",
    "- Seleziona variabili significative per il modello, migliorando l'interpretabilit√†.\n",
    "\n",
    "### **AIC vs BIC**\n",
    "| **Criterio**         | **AIC**                              | **BIC**                              |\n",
    "|:--------------------:|:------------------------------------:|:------------------------------------:|\n",
    "| **Formula**          | $-2\\ln(L) + 2k$                      | $-2\\ln(L) + k\\cdot\\ln(n)$            |\n",
    "| **Approccio**        | Basato sulla predizione futura       | Basato sull'inferenza bayesiana      |\n",
    "| **Bias**             | Favorisce modelli complessi          | Favorisce modelli semplici           |\n",
    "| **Dataset grande**   | Pi√π permissivo                       | Penalizza severamente modelli complessi  |\n",
    "| **Dataset piccolo**  | Pi√π affidabile per evitare underfitting  | Pi√π soggetto a underfitting          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backward_stepwise(t_var, n_vars, c_vars, df, tolerance=1e-6):\n",
    "    \n",
    "    # inizializzazione della formula con tutte le variabili\n",
    "    best_metric = smf.ols(\n",
    "        formula=build_formula(t_var, n_vars, c_vars),\n",
    "        data=df\n",
    "    ).fit().aic\n",
    "    current_vars = n_vars + c_vars\n",
    "    print(f\"AIC del modello completo: {round(best_metric, 2)}\")\n",
    "\n",
    "    while True:\n",
    "        summary = pd.DataFrame()\n",
    "\n",
    "        # calcolo dell'AIC per ogni possibile variabile da rimuovere\n",
    "        for var in current_vars:\n",
    "            vars_i = current_vars.copy()\n",
    "            vars_i.remove(var)\n",
    "            formula = build_formula(\n",
    "                t_var,\n",
    "                [v for v in vars_i if v in n_vars],\n",
    "                [v for v in vars_i if v in c_vars]\n",
    "            )\n",
    "            metric_i = smf.ols(formula=formula, data=df).fit().aic\n",
    "            summary = pd.concat(\n",
    "                [\n",
    "                    summary,\n",
    "                    pd.DataFrame({\"Removed variable\": [var], \"AIC\": [metric_i]})\n",
    "                ],\n",
    "                ignore_index=True\n",
    "            )\n",
    "        \n",
    "        # individuazione della variabile con l'AIC migliore\n",
    "        best_candidate_idx = summary['AIC'].idxmin()\n",
    "        best_candidate_aic = summary.loc[best_candidate_idx, \"AIC\"]\n",
    "        var_to_remove = summary.loc[best_candidate_idx, \"Removed variable\"]\n",
    "        \n",
    "        # controllo: l'AIC √® migliorato?\n",
    "        if best_candidate_aic < best_metric - tolerance:\n",
    "            best_metric = best_candidate_aic\n",
    "            current_vars.remove(var_to_remove)\n",
    "            print(f\"- Variabile rimossa (AIC = {round(best_metric, 2)}): {var_to_remove}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"Variabili scelte:\", sorted(current_vars))\n",
    "    return [var for var in current_vars if var in n_vars], [var for var in current_vars if var in c_vars]\n",
    "\n",
    "ch_num_vars, ch_cat_vars = run_backward_stepwise(target_var, num_vars.copy(), cat_vars.copy(), tr_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rimozione della multicollinearit√† tramite VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il **VIF** (Variance Inflation Factor) consente di valutare la presenza di multicollinearit√† tra le variabili indipendenti in un modello di regressione.\n",
    "\n",
    "### **Formula:**\n",
    "$\\text{VIF}_i = \\frac{1}{1 - R_i^2}$\n",
    "- $R_i^2$ rapresenta il coefficiente di determinazione della regressione della $i$-esima variabile indipendente sulle altre variabili indipendenti.\n",
    "\n",
    "### **Interpretazione**\n",
    "- $\\text{VIF}_i = 1$: nessuna correlazione con le altre variabili.\n",
    "- $1 < \\text{VIF}_i \\leq 5$: bassa correlazione, generalmente accettabile.\n",
    "- $\\text{VIF}_i > 5$: potenziale multicollinearit√†, da investigare.\n",
    "- $\\text{VIF}_i > 10$: multicollinearit√† severa, il modello potrebbe essere distorto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VIF_var_selection(n_vars, df):\n",
    "    \n",
    "    # inizializzazione: tutti i regressori numerici\n",
    "    print(f\"Variabili numeriche iniziali: {sorted(n_vars)}\")\n",
    "    current_vars = n_vars\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # calcolo del VIF per ogni covariata\n",
    "        subset = df[current_vars]\n",
    "        summary = pd.DataFrame()\n",
    "        summary[\"Variable\"] = subset.columns\n",
    "        summary[\"VIF\"] = [variance_inflation_factor(subset.values, i) for i in range(len(subset.columns))]\n",
    "        \n",
    "        # il regressore con il VIF peggiore viene rimosso\n",
    "        if summary[\"VIF\"].max() > 5:\n",
    "            print(f\"- Variabile rimossa (VIF = {round(summary[\"VIF\"].max(),2)}): {summary.loc[summary[\"VIF\"].idxmax(), \"Variable\"]}\")\n",
    "            current_vars.remove(summary.loc[summary[\"VIF\"].idxmax(), \"Variable\"])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f\"Variabili numeriche scelte:\", sorted(current_vars))\n",
    "    return current_vars\n",
    "\n",
    "ch_num_vars = VIF_var_selection(ch_num_vars.copy(), tr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stima di un modello di regressione multipla üëä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhat = smf.ols(\n",
    "    formula=build_formula(target_var, ch_num_vars, ch_cat_vars),\n",
    "    data=tr_train_data\n",
    ").fit()\n",
    "mhat.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi dei residui üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "fig.subplots_adjust(\n",
    "    wspace=.3,\n",
    "    hspace=.45\n",
    ")\n",
    "\n",
    "residuals = mhat.resid\n",
    "\n",
    "# test di normalit√† Jarque-Bera\n",
    "jb, value = scipy.stats.jarque_bera(residuals)\n",
    "sns.kdeplot(\n",
    "    x=mhat.resid,\n",
    "    color=\"dodgerblue\",\n",
    "    fill=True,\n",
    "    alpha=.75,\n",
    "    ax=axes[0]\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=np.random.normal(loc=residuals.mean(), scale=residuals.std(),size=100000),\n",
    "    color=\"gold\",\n",
    "    fill=True,\n",
    "    alpha=.25,\n",
    "    label=\"$N(\" + str(round(residuals.mean(), 2)) + \"; \" + str(round(residuals.std(), 2)) + \")$\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\n",
    "    \"$\\\\mathbf{Test \\\\ di \\\\ Jarque-Bera}$\\n$JB=\" + str(round(jb, 2)) + \"$, $p-value=\" + str(round(value*100, 2)) + \"\\\\%$, accetto H0\",\n",
    "    size=11\n",
    ")\n",
    "axes[0].set(\n",
    "    xlabel=\"Residuo trasf.\",\n",
    "    ylabel=\"Densit√†\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[0].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5,\n",
    ")\n",
    "axes[0].legend(\n",
    "    loc=\"upper right\"\n",
    ")\n",
    "\n",
    "# boxen-plot dei residui\n",
    "sns.boxenplot(\n",
    "    y=residuals,\n",
    "    color=\"dodgerblue\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"$\\\\mathbf{Boxen-plot \\\\ dei \\\\ residui}$\\n$mediana=\" + str(round(residuals.median(), 2)) + \"$\",\n",
    "    size=11\n",
    ")\n",
    "axes[1].set(\n",
    "    ylabel=\"Residuo trasf.\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[1].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "\n",
    "# test di omoschedasticit√† Breusch-Pagan\n",
    "_, pvalue, _, _ = het_breuschpagan(residuals, mhat.model.exog) \n",
    "sns.scatterplot(\n",
    "    x=tr_train_data[\"price_usd\"],\n",
    "    y=residuals,\n",
    "    color=\"dodgerblue\",\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\n",
    "    \"$\\\\mathbf{Test \\\\ di \\\\ omoschedasticit√†}$\\n$p-value=\" + str(round(pvalue*100, 2)) + \"\\\\%$, rifiuto H0\",\n",
    "    size=11\n",
    ")\n",
    "axes[2].set(\n",
    "    ylabel=\"Residuo trasf.\",\n",
    "    xlabel=\"Prezzo di vendita trasf.\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[2].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "\n",
    "# test di indipendenza Durbin-Watson\n",
    "dw_statistic = durbin_watson(residuals)\n",
    "plot_acf(\n",
    "    residuals,\n",
    "    lags=20,\n",
    "    color=\"dodgerblue\",\n",
    "    ax=axes[3]\n",
    ")\n",
    "axes[3].set_title(\n",
    "    \"$\\\\mathbf{Test \\\\ di \\\\ indipendeza}$\\n$statistica-dw=\" + str(round(dw_statistic, 2)) + \"$\",\n",
    "    size=11\n",
    ")\n",
    "axes[3].set(\n",
    "    xlabel=\"Lag\",\n",
    "    ylabel=\"Autocorrelazione\",\n",
    "    ylim=(-.25, 1.25),\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[3].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo della leverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistica e in particolare nell'analisi dei residui di un modello di regressione, la **leverage** √® una misura di quanto i valori delle variabili indipendenti relative a un'osservazione sono lontani da quelli assunti dalle altre osservazioni. I punti a elevata leverage, se presenti, sono outlier rispetto alle variabili indipendenti.\n",
    "\n",
    "### **Formula**\n",
    "$\\text{h}_{ii} = \\text{H[i, i]}$\n",
    "- $\\text{H} = \\text{X}(\\text{X}^T\\text{X})^{-1}\\text{X}^T$ √® la matrice di proiezione ortogonale.\n",
    "\n",
    "### **Interpretazione**\n",
    "- $\\text{h}_{ii} > \\frac{2(k+1)}{\\text{N}}$: l'osservazione $i$-esima √® un outlier capace di influenzare il piano di regressione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo della soglia e della leverage per ogni osservazione\n",
    "th = 2*(mhat.df_model+1)/len(tr_test_data)\n",
    "tr_train_data[\"leverage\"] = mhat.get_influence().hat_matrix_diag\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=2,\n",
    "    nrows=1,\n",
    "    figsize=(16, 5)\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=tr_train_data,\n",
    "    x=\"price_usd\",\n",
    "    y=residuals,\n",
    "    hue=tr_train_data[\"leverage\"],\n",
    "    palette=\"crest\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=tr_train_data[tr_train_data.leverage >= th],\n",
    "    x=\"price_usd\",\n",
    "    y=residuals,\n",
    "    marker=\"x\",\n",
    "    s=100,\n",
    "    color=\"black\",\n",
    "    label=\"Outlier\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\n",
    "    \"$\\\\mathbf{Leverage \\\\ di \\\\ ogni \\\\ dato \\\\ di \\\\ train}$\\n$th=\" + str(round(th, 2)) + \"$\",\n",
    "    size=11\n",
    ")\n",
    "axes[0].set(\n",
    "    xlabel=\"Prezzo di vendita trasf.\",\n",
    "    ylabel=\"Residuo trasf.\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[0].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=tr_train_data[\"leverage\"],\n",
    "    fill=True,\n",
    "    color=\"dodgerblue\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].axvline(\n",
    "    th,\n",
    "    color=\"orangered\",\n",
    "    ls=\"-.\",\n",
    "    label=\"Soglia\"\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"$\\\\mathbf{Distribuzione \\\\ delle \\\\ leverage}$\\n$mediana=\" + str(round(tr_train_data.leverage.median(), 2)) + \"$\",\n",
    "    size=11\n",
    ")\n",
    "axes[1].set(\n",
    "    xlabel=\"Leverage\",\n",
    "    ylabel=\"Densit√†\",\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[1].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "axes[1].legend();\n",
    "\n",
    "# rimozione degli outlier\n",
    "# tr_train_data = tr_train_data[tr_train_data.leverage < th]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Torna al training del modello](#stepwise-backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validazione del modello üçÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "real_price_usd = np.exp(scaler[\"price_usd\"].inverse_transform(np.array(tr_train_data[\"price_usd\"]).reshape(-1, 1)))\n",
    "predicted_price_usd = np.exp(scaler[\"price_usd\"].inverse_transform(np.array(mhat.fittedvalues).reshape(-1, 1)))\n",
    "train_RMSE = root_mean_squared_error(\n",
    "    real_price_usd,\n",
    "    predicted_price_usd\n",
    ")\n",
    "print(f\"RMSE di train: {round(train_RMSE, 2)} $\")\n",
    "\n",
    "# test\n",
    "real_price_usd = np.exp(scaler[\"price_usd\"].inverse_transform(np.array(tr_test_data[\"price_usd\"]).reshape(-1, 1)))\n",
    "predicted_price_usd = np.exp(scaler[\"price_usd\"].inverse_transform(np.array(mhat.predict(tr_test_data)).reshape(-1, 1)))\n",
    "test_RMSE = root_mean_squared_error(\n",
    "    real_price_usd,\n",
    "    predicted_price_usd\n",
    ")\n",
    "print(f\"RMSE di test: {round(test_RMSE, 2)} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approfondimento: le reti neurali (NN) con `tensorflow`üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una **rete neurale** √® un modello computazionale ispirato al funzionamento del cervello umano, progettato per elaborare dati e apprendere schemi complessi. √à composta da **neuroni artificiali**, organizzati in **strati**:\n",
    "\n",
    "1. **Strato di input**: riceve i dati iniziali.\n",
    "2. **Strati nascosti**: elaborano le informazioni attraverso operazioni matematiche e funzioni di attivazione.\n",
    "3. **Strato di output**: restituisce il risultato finale.\n",
    "\n",
    "Ogni connessione tra i neuroni ha un **peso**, che viene ottimizzato durante l'addestramento per migliorare le prestazioni del modello. Questo tipo di rete √® usato in compiti come classificazione, regressione e predizione.\n",
    "\n",
    "### **Modello**\n",
    "<img src=\"https://www.researchgate.net/profile/Ali-Polat-9/publication/322276434/figure/fig2/AS:623915986587648@1525764571315/Structure-of-artificial-neuron.png\">\n",
    "\n",
    "### **Esempio**\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:800/0*-7I6evZetK0o_hq0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione dei pacchetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Dense, Normalization, Input, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codifica One-Hot delle variabili categoriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = pd.get_dummies(\n",
    "    data=data,\n",
    "    columns=cat_vars,\n",
    "    drop_first=True\n",
    ").astype(float)\n",
    "encoded_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suddivisione del dataset in train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoded_data.drop(columns=\"price_usd\")\n",
    "y = encoded_data[\"price_usd\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=.3,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definizione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = load_model(\"NN.keras\")\n",
    "except:\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(X_train.shape[1], )),\n",
    "            Normalization(),\n",
    "            Dense(16),\n",
    "            Dropout(.25),\n",
    "            Dense(8),\n",
    "            Dense(1)\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mean_squared_error\"\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stima del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzioni di callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=500,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"NN.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    mode=\"min\",\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10000,\n",
    "    batch_size=64,\n",
    "    validation_split=.3,\n",
    "    callbacks=[\n",
    "        checkpoint,\n",
    "        early_stop\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "best_model = load_model(\"NN.keras\")\n",
    "y_pred_train = best_model.predict(X_train).reshape(-1)\n",
    "test_RMSE = root_mean_squared_error(\n",
    "    y_true=np.array(y_train),\n",
    "    y_pred=y_pred_train\n",
    ")\n",
    "print(f\"RMSE di train: {round(test_RMSE, 2)} $\")\n",
    "\n",
    "# test\n",
    "best_model = load_model(\"NN.keras\")\n",
    "y_pred_test = best_model.predict(X_test).reshape(-1)\n",
    "test_RMSE = root_mean_squared_error(\n",
    "    y_true=np.array(y_test),\n",
    "    y_pred=y_pred_test\n",
    ")\n",
    "print(f\"RMSE di test: {round(test_RMSE, 2)} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuzione dei residui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ncols=2,\n",
    "    nrows=1,\n",
    "    figsize=(16, 5)\n",
    ")\n",
    "\n",
    "# train\n",
    "sns.histplot(\n",
    "    x = np.array(y_train) - y_pred_train,\n",
    "    color=\"dodgerblue\",\n",
    "    stat=\"percent\",\n",
    "    kde=True,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\n",
    "    \"$\\\\mathbf{Distribuzione \\\\ dei \\\\ residui \\\\ di \\\\ train}$\",\n",
    "    size=11\n",
    ")\n",
    "axes[0].set(\n",
    "    xlabel=\"Residuo [¬∞C]\",\n",
    "    ylabel=\"Percentuale [%]\",\n",
    "    xlim=(-1000, 1000),\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[0].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")\n",
    "\n",
    "# test\n",
    "sns.histplot(\n",
    "    x = np.array(y_test) - y_pred_test,\n",
    "    color=\"orangered\",\n",
    "    stat=\"percent\",\n",
    "    kde=True,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"$\\\\mathbf{Distribuzione \\\\ dei \\\\ residui \\\\ di \\\\ test}$\",\n",
    "    size=11\n",
    ")\n",
    "axes[1].set(\n",
    "    xlabel=\"Residuo [¬∞C]\",\n",
    "    ylabel=\"Percentuale [%]\",\n",
    "    xlim=(-1000, 1000),\n",
    "    facecolor=\"whitesmoke\"\n",
    ")\n",
    "axes[1].grid(\n",
    "    ls=\"--\",\n",
    "    alpha=.5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
